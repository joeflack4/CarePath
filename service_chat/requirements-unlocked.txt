fastapi
uvicorn
httpx
pydantic-settings

# LLM dependencies (for LLM_MODE=qwen or Qwen3-4B-Thinking-2507) - SLOW on CPU
torch
# Qwen3 models require transformers >= 4.51.0 (Qwen3 support was added in 4.51)
transformers
huggingface-hub
accelerate

# GGUF/llama.cpp dependencies (for LLM_MODE=gguf) - FAST on CPU
llama-cpp-python
